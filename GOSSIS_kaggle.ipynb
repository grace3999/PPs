{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#getting and working with data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import os\n",
    "import scipy as sp\n",
    "import missingno as msno\n",
    "\n",
    "#visualizing results\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "sns.set_context('poster', rc={'font.size':35,\n",
    "                              'axes.titlesize':50,\n",
    "                              'axes.labelsize':35})\n",
    "\n",
    "#machine learning\n",
    "import category_encoders as ce\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer, LabelEncoder, PolynomialFeatures\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, GroupKFold, train_test_split, cross_val_score, cross_val_predict, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, AdaBoostClassifier\n",
    "from sklearn.svm import SVC \n",
    "#import xgboost as xgb\n",
    "from sklearn.neighbors import KNeighborsClassifier \n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import auc, accuracy_score, confusion_matrix, mean_squared_error, roc_auc_score, classification_report\n",
    "\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 500)\n",
    "pd.set_option('display.width', 1000)\n",
    "\n",
    "import warnings; warnings.simplefilter('ignore')\n",
    "np.set_printoptions(suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_path = 'C:/Users/Schindler/Documents/ProgrammingFun/GOSSIS_kaggle/training_v2.csv'\n",
    "unlabeled_data_path = 'C:/Users/Schindler/Documents/ProgrammingFun/GOSSIS_kaggle/unlabeled.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(train_data_path)\n",
    "data = pd.DataFrame(data = data)\n",
    "\n",
    "print('Original data shape:\\n', data.shape, '\\n')\n",
    "print('Group value counts:\\n', data['hospital_death'].value_counts(), '\\n')\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lots of missing data\n",
    "data.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore missing data \n",
    "na_0_mean = (data[data['hospital_death'] == 0].isna().sum() / data[data['hospital_death'] == 0].shape[0] * 100).mean()\n",
    "na_1_mean = (data[data['hospital_death'] == 1].isna().sum() / data[data['hospital_death'] == 1].shape[0] * 100).mean()\n",
    "print(na_0_mean, na_1_mean)\n",
    "#there is a bit more missing data for the entries that lived (not surprizing as you get more tests the poorer health)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#are there parameters that have more missing values for yes vs no death\n",
    "na_0 = (data[data['hospital_death'] == 0].isna().sum() / data[data['hospital_death'] == 0].shape[0]) * 100\n",
    "na_1 = (data[data['hospital_death'] == 1].isna().sum() / data[data['hospital_death'] == 1].shape[0]) * 100\n",
    "na_0_diff = data[data['hospital_death'] == 0].shape[0] - data[data['hospital_death'] == 0].isna().sum()\n",
    "na_1_diff = data[data['hospital_death'] == 1].shape[0] - data[data['hospital_death'] == 1].isna().sum()\n",
    "data_na_perc = pd.DataFrame(data=[na_0, na_1, na_0_diff, na_1_diff])\n",
    "data_na_perc = data_na_perc.T.sort_values(by=0, ascending=False)\n",
    "data_na_perc['diff'] = data_na_perc[0] - data_na_perc[1]\n",
    "data_na_perc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#organize param names\n",
    "\n",
    "data_meta = ['encounter_id', 'patient_id', 'hospital_id', 'icu_id']\n",
    "\n",
    "param_cat = ['ethnicity', 'gender', 'icu_admit_source', 'hospital_admit_source', 'icu_stay_type', 'icu_type', \n",
    "       'apache_3j_bodysystem', 'apache_2_bodysystem']\n",
    "\n",
    "param_baics_apache = ['age', 'bmi', 'height', 'weight', 'pre_icu_los_days',\n",
    "                      'apache_2_diagnosis', 'apache_3j_diagnosis', 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob',\n",
    " 'albumin_apache', 'bilirubin_apache', 'bun_apache', 'creatinine_apache', 'fio2_apache',\n",
    "       'gcs_eyes_apache', 'gcs_motor_apache', 'gcs_unable_apache',\n",
    "       'gcs_verbal_apache', 'glucose_apache', 'heart_rate_apache',\n",
    "       'hematocrit_apache',  'map_apache',\n",
    "       'paco2_apache', 'paco2_for_ph_apache', 'pao2_apache', 'ph_apache',\n",
    "       'resprate_apache', 'sodium_apache', 'temp_apache',\n",
    "       'urineoutput_apache', 'ventilated_apache', 'wbc_apache', 'elective_surgery', 'readmission_status', 'arf_apache', 'apache_post_operative', 'intubated_apache',\n",
    "        'aids', 'cirrhosis', 'diabetes_mellitus', 'hepatic_failure',\n",
    "       'immunosuppression', 'leukemia', 'lymphoma',\n",
    "       'solid_tumor_with_metastasis']\n",
    " \n",
    "param_vitals = ['d1_diasbp_invasive_max', 'd1_diasbp_invasive_min',\n",
    "       'd1_diasbp_max', 'd1_diasbp_min', 'd1_diasbp_noninvasive_max',\n",
    "       'd1_diasbp_noninvasive_min', 'd1_heartrate_max',\n",
    "       'd1_heartrate_min', 'd1_mbp_invasive_max', 'd1_mbp_invasive_min',\n",
    "       'd1_mbp_max', 'd1_mbp_min', 'd1_mbp_noninvasive_max',\n",
    "       'd1_mbp_noninvasive_min', 'd1_resprate_max', 'd1_resprate_min',\n",
    "       'd1_spo2_max', 'd1_spo2_min', 'd1_sysbp_invasive_max',\n",
    "       'd1_sysbp_invasive_min', 'd1_sysbp_max', 'd1_sysbp_min',\n",
    "       'd1_sysbp_noninvasive_max', 'd1_sysbp_noninvasive_min',\n",
    "       'd1_temp_max', 'd1_temp_min', 'h1_diasbp_invasive_max',\n",
    "       'h1_diasbp_invasive_min', 'h1_diasbp_max', 'h1_diasbp_min',\n",
    "       'h1_diasbp_noninvasive_max', 'h1_diasbp_noninvasive_min',\n",
    "       'h1_heartrate_max', 'h1_heartrate_min', 'h1_mbp_invasive_max',\n",
    "       'h1_mbp_invasive_min', 'h1_mbp_max', 'h1_mbp_min',\n",
    "       'h1_mbp_noninvasive_max', 'h1_mbp_noninvasive_min',\n",
    "       'h1_resprate_max', 'h1_resprate_min', 'h1_spo2_max', 'h1_spo2_min',\n",
    "       'h1_sysbp_invasive_max', 'h1_sysbp_invasive_min', 'h1_sysbp_max',\n",
    "       'h1_sysbp_min', 'h1_sysbp_noninvasive_max',\n",
    "       'h1_sysbp_noninvasive_min', 'h1_temp_max', 'h1_temp_min']\n",
    "       \n",
    "param_labs = ['d1_albumin_max', 'd1_albumin_min', 'd1_bilirubin_max',\n",
    "       'd1_bilirubin_min', 'd1_bun_max', 'd1_bun_min', 'd1_calcium_max',\n",
    "       'd1_calcium_min', 'd1_creatinine_max', 'd1_creatinine_min',\n",
    "       'd1_glucose_max', 'd1_glucose_min', 'd1_hco3_max', 'd1_hco3_min',\n",
    "       'd1_hemaglobin_max', 'd1_hemaglobin_min', 'd1_hematocrit_max',\n",
    "       'd1_hematocrit_min', 'd1_inr_max', 'd1_inr_min', 'd1_lactate_max',\n",
    "       'd1_lactate_min', 'd1_platelets_max', 'd1_platelets_min',\n",
    "       'd1_potassium_max', 'd1_potassium_min', 'd1_sodium_max',\n",
    "       'd1_sodium_min', 'd1_wbc_max', 'd1_wbc_min', 'h1_albumin_max',\n",
    "       'h1_albumin_min', 'h1_bilirubin_max', 'h1_bilirubin_min',\n",
    "       'h1_bun_max', 'h1_bun_min', 'h1_calcium_max', 'h1_calcium_min',\n",
    "       'h1_creatinine_max', 'h1_creatinine_min', 'h1_glucose_max',\n",
    "       'h1_glucose_min', 'h1_hco3_max', 'h1_hco3_min',\n",
    "       'h1_hemaglobin_max', 'h1_hemaglobin_min', 'h1_hematocrit_max',\n",
    "       'h1_hematocrit_min', 'h1_inr_max', 'h1_inr_min', 'h1_lactate_max',\n",
    "       'h1_lactate_min', 'h1_platelets_max', 'h1_platelets_min',\n",
    "       'h1_potassium_max', 'h1_potassium_min', 'h1_sodium_max',\n",
    "       'h1_sodium_min', 'h1_wbc_max', 'h1_wbc_min']\n",
    "\n",
    "param_labs_blood = ['d1_arterial_pco2_max', 'd1_arterial_pco2_min',\n",
    "       'd1_arterial_ph_max', 'd1_arterial_ph_min', 'd1_arterial_po2_max',\n",
    "       'd1_arterial_po2_min', 'd1_pao2fio2ratio_max',\n",
    "       'd1_pao2fio2ratio_min', 'h1_arterial_pco2_max',\n",
    "       'h1_arterial_pco2_min', 'h1_arterial_ph_max', 'h1_arterial_ph_min',\n",
    "       'h1_arterial_po2_max', 'h1_arterial_po2_min',\n",
    "       'h1_pao2fio2ratio_max', 'h1_pao2fio2ratio_min']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature selection for continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#explore autocorrelation across data set\n",
    "corr = data[param_baics_apache].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(31, 31))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=\"YlGnBu\", robust=True, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove non-invasive and invasive (highly corr with regular measure), only use d1 (not h1 - lots of missing and corr with d1)\n",
    "features_comb = ['hospital_death', 'ethnicity', 'gender', 'icu_admit_source', 'hospital_admit_source', 'icu_stay_type', 'icu_type', \n",
    "       'apache_3j_bodysystem', 'apache_2_bodysystem', \n",
    "                 'age', 'bmi', \n",
    "                 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob',\n",
    "                 'fio2_apache',\n",
    "                 'gcs_eyes_apache', 'gcs_motor_apache', 'gcs_unable_apache', 'gcs_verbal_apache', \n",
    "                 'urineoutput_apache', 'ventilated_apache', 'elective_surgery', 'intubated_apache',\n",
    "                 'aids', 'cirrhosis', 'diabetes_mellitus', 'hepatic_failure', \n",
    "                 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis',\n",
    "                 'd1_diasbp_max', 'd1_diasbp_min', \n",
    "                   'd1_heartrate_max', 'd1_heartrate_min', \n",
    "                   'd1_mbp_max', 'd1_mbp_min', \n",
    "                   'd1_resprate_max', 'd1_resprate_min',\n",
    "                   'd1_spo2_max', 'd1_spo2_min', \n",
    "                   'd1_sysbp_max', 'd1_sysbp_min',\n",
    "                   'd1_temp_max', 'd1_temp_min',\n",
    "                 'd1_albumin_max', 'd1_albumin_min', \n",
    "              'd1_bilirubin_max', 'd1_bilirubin_min', \n",
    "              'd1_bun_max', 'd1_bun_min', \n",
    "              'd1_calcium_max', 'd1_calcium_min', \n",
    "              'd1_creatinine_max', 'd1_creatinine_min',\n",
    "              'd1_glucose_max', 'd1_glucose_min', \n",
    "              'd1_hco3_max', 'd1_hco3_min',\n",
    "              'd1_hemaglobin_max', 'd1_hemaglobin_min', \n",
    "              'd1_hematocrit_max', 'd1_hematocrit_min', \n",
    "              'd1_inr_max', 'd1_inr_min', \n",
    "              'd1_lactate_max', 'd1_lactate_min', \n",
    "              'd1_platelets_max', 'd1_platelets_min',\n",
    "              'd1_potassium_max', 'd1_potassium_min', \n",
    "              'd1_sodium_max', 'd1_sodium_min', \n",
    "              'd1_wbc_max', 'd1_wbc_min',\n",
    "                 'd1_arterial_pco2_max', 'd1_arterial_pco2_min',\n",
    "                    'd1_arterial_ph_max', 'd1_arterial_ph_min', \n",
    "                    'd1_arterial_po2_max', 'd1_arterial_po2_min', \n",
    "                    'd1_pao2fio2ratio_max', 'd1_pao2fio2ratio_min']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#explore autocorrelation across data set\n",
    "corr = data[features_comb].corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(33, 19))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=\"YlGnBu\", robust=True, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Categorical viz and encoding - cat variables are important in health so want to encode using model instead of one-hot etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in data[param_cat]:\n",
    "    print(param)\n",
    "\n",
    "    data_int = (data.groupby('hospital_death')[param].value_counts() /\n",
    "                        data.groupby('hospital_death')[param].count()).reset_index(name='perc')\n",
    "        \n",
    "    try:\n",
    "        g = sns.catplot(x=param, y='perc', kind='bar', data=data_int, hue='hospital_death', ci=68, height=5, aspect=4)\n",
    "        plt.show()\n",
    "        \n",
    "        print('\\n')\n",
    "        \n",
    "    except:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#encode categorical variables using m_estimate\n",
    "data_cat_train = data[features_comb]\n",
    "\n",
    "Y_train = data_cat_train['hospital_death']\n",
    "X_train = data_cat_train[features_comb]\n",
    "\n",
    "\n",
    "# use target encoding to encode two categorical features\n",
    "enc = ce.TargetEncoder(cols=param_cat)\n",
    "cat_enc_data = enc.fit_transform(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#explore autocorrelation across data set\n",
    "corr = cat_enc_data.corr()\n",
    "\n",
    "# Generate a mask for the upper triangle\n",
    "mask = np.triu(np.ones_like(corr, dtype=np.bool))\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "f, ax = plt.subplots(figsize=(33, 19))\n",
    "\n",
    "# Draw the heatmap with the mask and correct aspect ratio\n",
    "sns.heatmap(corr, mask=mask, cmap=\"YlGnBu\", robust=True, square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Impute missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use missingo to viz missing data\n",
    "msno.heatmap(cat_enc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#impute missing data for features\n",
    "\n",
    "params = param_baics_apache\n",
    "imp = IterativeImputer(max_iter=9, random_state=39, verbose=2)\n",
    "cat_enc_data_imp = imp.fit_transform(cat_enc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = pd.DataFrame(data=cat_enc_data_imp, columns=cat_enc_data.columns.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.to_csv('final_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['ethnicity', 'gender', 'icu_admit_source', 'hospital_admit_source', 'icu_stay_type', 'icu_type', \n",
    "       'apache_3j_bodysystem', 'apache_2_bodysystem', \n",
    "                 'age', 'bmi', \n",
    "                 'apache_4a_hospital_death_prob', 'apache_4a_icu_death_prob',\n",
    "                 'fio2_apache',\n",
    "                 'gcs_eyes_apache', 'gcs_motor_apache', 'gcs_unable_apache', 'gcs_verbal_apache', \n",
    "                 'urineoutput_apache', 'ventilated_apache', 'elective_surgery', 'intubated_apache',\n",
    "                 'aids', 'cirrhosis', 'diabetes_mellitus', 'hepatic_failure', \n",
    "                 'immunosuppression', 'leukemia', 'lymphoma', 'solid_tumor_with_metastasis',\n",
    "                 'd1_diasbp_max', 'd1_diasbp_min', \n",
    "                   'd1_heartrate_max', 'd1_heartrate_min', \n",
    "                   'd1_mbp_max', 'd1_mbp_min', \n",
    "                   'd1_resprate_max', 'd1_resprate_min',\n",
    "                   'd1_spo2_max', 'd1_spo2_min', \n",
    "                   'd1_sysbp_max', 'd1_sysbp_min',\n",
    "                   'd1_temp_max', 'd1_temp_min',\n",
    "                 'd1_albumin_max', 'd1_albumin_min', \n",
    "              'd1_bilirubin_max', 'd1_bilirubin_min', \n",
    "              'd1_bun_max', 'd1_bun_min', \n",
    "              'd1_calcium_max', 'd1_calcium_min', \n",
    "              'd1_creatinine_max', 'd1_creatinine_min',\n",
    "              'd1_glucose_max', 'd1_glucose_min', \n",
    "              'd1_hco3_max', 'd1_hco3_min',\n",
    "              'd1_hemaglobin_max', 'd1_hemaglobin_min', \n",
    "              'd1_hematocrit_max', 'd1_hematocrit_min', \n",
    "              'd1_inr_max', 'd1_inr_min', \n",
    "              'd1_lactate_max', 'd1_lactate_min', \n",
    "              'd1_platelets_max', 'd1_platelets_min',\n",
    "              'd1_potassium_max', 'd1_potassium_min', \n",
    "              'd1_sodium_max', 'd1_sodium_min', \n",
    "              'd1_wbc_max', 'd1_wbc_min',\n",
    "                 'd1_arterial_pco2_max', 'd1_arterial_pco2_min',\n",
    "                    'd1_arterial_ph_max', 'd1_arterial_ph_min', \n",
    "                    'd1_arterial_po2_max', 'd1_arterial_po2_min', \n",
    "                    'd1_pao2fio2ratio_max', 'd1_pao2fio2ratio_min']\n",
    "\n",
    "#split data\n",
    "train, test = train_test_split(final_data, test_size = .3, random_state=1, stratify = final_data['hospital_death'])\n",
    "\n",
    "Y_train = train['hospital_death']\n",
    "Y_test = test['hospital_death']\n",
    "\n",
    "\n",
    "X_train = train[features]\n",
    "X_test = test[features]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data algo\n",
    "scaler = StandardScaler()\n",
    "\n",
    "#k fold algo\n",
    "strat_k_fold = StratifiedKFold(n_splits=10)\n",
    "\n",
    "#classifier algos\n",
    "dm_cv = DummyClassifier(strategy='stratified', random_state=39)\n",
    "lr_cv = LogisticRegression(random_state=39, class_weight='balanced')\n",
    "rf_cv = RandomForestClassifier(random_state=39, class_weight='balanced')\n",
    "svm_cv = SVC(kernel='linear', probability=True, class_weight='balanced') \n",
    "knn_cv = KNeighborsClassifier()\n",
    "ab_cv = AdaBoostClassifier(random_state=39)\n",
    "\n",
    "#dic with classifier and feature importance attribute name\n",
    "models_dic = {'dm_cv': (dm_cv, 'none'), \n",
    "              'lr_cv': (lr_cv, 'coef'), \n",
    "              'rf_cv': (rf_cv, 'feature_importance'), \n",
    "              'svm_cv':(svm_cv, 'coef'), \n",
    "              'knn_cv': (knn_cv, 'none'),  \n",
    "              'ab_cv': (ab_cv, 'feature_importance')}\n",
    "\n",
    "#gb_cv = GradientBoostingClassifier(random_state=39)\n",
    "#'gb_cv': (gb_cv, 'feature_importance'),"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def feature_importance(X, y, model_instance, feature_names, fi_name):\n",
    "    #takes in features (X) and classess (y), model, column names for features in X, and name of attribute for feature importance\n",
    "    #returns dictionary of feature names and coef/feature importance values\n",
    "    \n",
    "    feature_importance_dic = {}\n",
    "    \n",
    "    model_instance.fit(X, y)\n",
    "    \n",
    "    if fi_name == 'coef':\n",
    "        coef = model_instance.coef_[0]\n",
    "        feature_importance_dic = dict(zip(feature_names, coef))\n",
    "    if fi_name == 'feature_importance':\n",
    "        coef = model_instance.feature_importances_\n",
    "        feature_importance_dic = dict(zip(feature_names, coef))\n",
    "    if fi_name == 'none':\n",
    "        coef = np.zeros(len(feature_names))\n",
    "        feature_importance_dic = dict(zip(feature_names, coef))\n",
    "    \n",
    "    return feature_importance_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def classification_pipeline(X, y, cv_instance, model_instance, feature_names, fi_name):\n",
    "    \n",
    "    #scale data\n",
    "    data_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    #generate cross-val sets\n",
    "    cv = list(cv_instance.split(data_scaled, y))\n",
    "    \n",
    "    #predict class and predict probability \n",
    "    y_pred = cross_val_predict(model_instance, data_scaled, y, cv=cv, method='predict')\n",
    "    y_pred_prob = cross_val_predict(model_instance, data_scaled, y, cv=cv, method='predict_proba')\n",
    "    \n",
    "    #generate confusion matrix\n",
    "    conf_mat = confusion_matrix(y, y_pred)\n",
    "    print('Confusion matrix:', conf_mat)\n",
    "    \n",
    "    #generate ROC_AUC\n",
    "    ROC_AUC = metrics.roc_auc_score(y, y_pred_prob[:,1])\n",
    "    print(\"ROC_AUC: \", ROC_AUC)\n",
    "    \n",
    "    # generate additional metrics\n",
    "    recall = metrics.recall_score(y,y_pred)\n",
    "    precision = metrics.precision_score(y,y_pred)\n",
    "    accuracy = metrics.accuracy_score(y,y_pred)\n",
    "    F1 = metrics.f1_score(y,y_pred)\n",
    "    print(\"Sensitivity/Recall (TPR): \",recall)\n",
    "    print(\"Precision (PPV): \", precision)\n",
    "    print(\"Accuracy: \", accuracy)\n",
    "    print(\"F1:\", F1)\n",
    "    \n",
    "    #determine feature importance\n",
    "    feature_dic = feature_importance(data_scaled, y, model_instance, feature_names, fi_name)\n",
    "    \n",
    "    #create dic\n",
    "    data_dic = {}\n",
    "    data_dic['y_pred'] = y_pred\n",
    "    data_dic['y_pred_prob'] = y_pred_prob\n",
    "    data_dic['conf_mat'] = conf_mat\n",
    "    data_dic['ROC_AUC'] = ROC_AUC\n",
    "    data_dic['recall'] = recall\n",
    "    data_dic['precision'] = precision\n",
    "    data_dic['accuracy'] = accuracy\n",
    "    data_dic['F1'] = F1\n",
    "    \n",
    "    data_dic = {**data_dic, **feature_dic}\n",
    "    \n",
    "    return data_dic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "feature_set = 'full'\n",
    "feature_names = features\n",
    "\n",
    "data_full_features = {}\n",
    "\n",
    "for name, model in models_dic.items():\n",
    "    print(f'{name} model with {feature_set} features:')\n",
    "    data_full_features[name + '_' + feature_set] = classification_pipeline(X_train, Y_train, strat_k_fold, model[0], feature_names, model[1])\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#put dics in pandas df \n",
    "final_dic = {**data_full_features}\n",
    "data_pandas = pd.DataFrame.from_dict(data = final_dic, orient='index')\n",
    "data_pandas.sort_values('precision', ascending=False).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fpr_dm, tpr_dm, thresholds_dm = metrics.roc_curve(Y_train_class, data_full_features['dm_cv_ave']['y_pred_prob'][:,1])\n",
    "fpr_lr, tpr_lr, thresholds_lr = metrics.roc_curve(Y_train_class, data_full_features['lr_cv_ave']['y_pred_prob'][:,1])\n",
    "fpr_rf, tpr_rf, thresholds_rf = metrics.roc_curve(Y_train_class, data_full_features['rf_cv_ave']['y_pred_prob'][:,1])\n",
    "fpr_gb, tpr_gb, thresholds_gb = metrics.roc_curve(Y_train_class, data_full_features['gb_cv_ave']['y_pred_prob'][:,1])\n",
    "fpr_svm, tpr_svm, thresholds_svm = metrics.roc_curve(Y_train_class, data_full_features['svm_cv_ave']['y_pred_prob'][:,1])\n",
    "fpr_knn, tpr_knn, thresholds_knn = metrics.roc_curve(Y_train_class, data_full_features['knn_cv_ave']['y_pred_prob'][:,1])\n",
    "fpr_ab, tpr_ab, thresholds_ab = metrics.roc_curve(Y_train_class, data_full_features['ab_cv_ave']['y_pred_prob'][:,1])\n",
    "\n",
    "# plot model ROC curves\n",
    "plt.plot(fpr_dm, tpr_dm, label=\"dm\")\n",
    "plt.plot(fpr_lr, tpr_lr, label=\"lr\")\n",
    "plt.plot(fpr_rf, tpr_rf, label=\"rf\")\n",
    "plt.plot(fpr_gb, tpr_gb, label=\"gb\")\n",
    "plt.plot(fpr_svm, tpr_svm, label=\"svm\")\n",
    "plt.plot(fpr_knn, tpr_knn, label=\"knn\")\n",
    "plt.plot(fpr_ab, tpr_ab, label=\"ada\")\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('False Positive Rate (1 - Specificity)', fontsize = 15)\n",
    "plt.ylabel('True Positive Rate (Sensitivity)', fontsize = 15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# calculate precision-recall curve\n",
    "precision_dm, recall_dm, thresholds_pr_dm = metrics.precision_recall_curve(Y_train_class, data_full_features['dm_cv_ave']['y_pred_prob'][:,1])\n",
    "precision_lr, recall_lr, thresholds_pr_lr = metrics.precision_recall_curve(Y_train_class, data_full_features['lr_cv_ave']['y_pred_prob'][:,1])\n",
    "precision_rf, recall_rf, thresholds_pr_rf = metrics.precision_recall_curve(Y_train_class, data_full_features['rf_cv_ave']['y_pred_prob'][:,1])\n",
    "precision_gb, recall_gb, thresholds_pr_gb = metrics.precision_recall_curve(Y_train_class, data_full_features['gb_cv_ave']['y_pred_prob'][:,1])\n",
    "precision_svm, recall_svm, thresholds_pr_svm = metrics.precision_recall_curve(Y_train_class, data_full_features['svm_cv_ave']['y_pred_prob'][:,1])\n",
    "precision_knn, recall_knn, thresholds_pr_knn = metrics.precision_recall_curve(Y_train_class, data_full_features['knn_cv_ave']['y_pred_prob'][:,1])\n",
    "precision_ab, recall_ab, thresholds_pr_ab = metrics.precision_recall_curve(Y_train_class, data_full_features['ab_cv_ave']['y_pred_prob'][:,1])\n",
    "\n",
    "plt.plot(recall_dm, precision_dm, label='dm')\n",
    "plt.plot(recall_lr, precision_lr, label='lr')\n",
    "plt.plot(recall_rf, precision_rf, label='rf')\n",
    "plt.plot(recall_gb, precision_gb, label='gb')\n",
    "plt.plot(recall_svm, precision_svm, label='svm')\n",
    "plt.plot(recall_knn, precision_knn, label='knn')\n",
    "plt.plot(recall_ab, precision_ab, label='ada')\n",
    "\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.xlabel('Recall (Sensitivity)', fontsize = 15)\n",
    "plt.ylabel('Precision', fontsize = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save final model using pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#pickel model to save for later use\n",
    "save_path = 'C:/Users/Schindler/Documents/Schindler_Lab/Data/Escalation/Ferguson/'\n",
    "\n",
    "pkl_filename = str(save_path + \"5day_ave_lr.pkl\")  \n",
    "with open(pkl_filename, 'wb') as file:  \n",
    "    pickle.dump(lr_cv, file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "grid searches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data for grid search\n",
    "train_scaled = scaler.fit_transform(X_train_full)\n",
    "\n",
    "#grid search with cv for gb and full features\n",
    "param_grid = {\n",
    "    \"loss\":[\"deviance\"],\n",
    "    \"learning_rate\": [0.01, 0.05, 0.1, 0.2],\n",
    "    \"min_samples_split\": np.linspace(0.1, 0.5, 4),\n",
    "    \"min_samples_leaf\": np.linspace(0.1, 0.5, 4),\n",
    "    \"max_depth\":[3,5,8],\n",
    "    \"max_features\":[\"log2\",\"sqrt\"],\n",
    "    \"criterion\": [\"friedman_mse\",  \"mae\"],\n",
    "    \"subsample\":[0.5, 0.8, 0.9, 1.0],\n",
    "    \"n_estimators\":[10]\n",
    "    }\n",
    "\n",
    "scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "gb_base = GradientBoostingClassifier(random_state=39)\n",
    "\n",
    "gb_gs = GridSearchCV(gb_base, param_grid, scoring='accuracy', cv=3, refit='f1')\n",
    "gb_gs.fit(train_scaled, Y_train_class)\n",
    "\n",
    "print(\"f1:\"+str(np.average(cross_val_score(gb_gs, train_scaled, Y_train_class, scoring='f1'))))\n",
    "print(\"ROC_AUC:\"+str(np.average(cross_val_score(gb_gs, train_scaled, Y_train_class, scoring='roc_auc'))))\n",
    "\n",
    "print(gb_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use best params\n",
    "train_scaled = scaler.fit_transform(X_train_full)\n",
    "\n",
    "gb_best = GradientBoostingClassifier(criterion='friedman_mse', learning_rate=0.1, loss='deviance', max_depth=3, max_features='log2', min_samples_leaf= 0.25, min_samples_split=0.1, n_estimators=10, subsample=0.8, random_state=39)\n",
    "    \n",
    "print(\"f1:\"+str(np.average(cross_val_score(gb_best, train_scaled, Y_train_class, scoring='f1'))))\n",
    "print(\"ROC_AUC:\"+str(np.average(cross_val_score(gb_best, train_scaled, Y_train_class, scoring='roc_auc'))))\n",
    "print(\"Accuracy:\"+str(np.average(cross_val_score(gb_best, train_scaled, Y_train_class, scoring='accuracy'))))\n",
    "\n",
    "gb_best.fit(train_scaled, Y_train_class)\n",
    "print(gb_best.score(train_scaled, Y_train_class))\n",
    "\n",
    "train_pred_gb = gb_best.predict(train_scaled)\n",
    "train_pred_prob_gb = gb_best.predict_proba(train_scaled)\n",
    "print(classification_report(Y_train_class, train_pred_gb))\n",
    "print(confusion_matrix(Y_train_class, train_pred_gb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#scale data for grid search\n",
    "train_scaled = scaler.fit_transform(X_train_ave)\n",
    "\n",
    "#grid search with cv for svm and ave features\n",
    "param_grid = {'C':(0.001, 0.01, 0.1, 1, 10), 'decision_function_shape':('ovo','ovr'), 'kernel':('linear', 'rbf')}\n",
    "scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "svm_base = SVC(class_weight='balanced', random_state=39)\n",
    "\n",
    "svm_gs = GridSearchCV(svm_base, param_grid, cv=3, scoring = scoring, refit='f1')\n",
    "svm_gs.fit(train_scaled, Y_train_class)\n",
    "\n",
    "print(\"f1:\"+str(np.average(cross_val_score(svm_gs, train_scaled, Y_train_class, scoring='f1'))))\n",
    "print(\"ROC_AUC:\"+str(np.average(cross_val_score(svm_gs, train_scaled, Y_train_class, scoring='roc_auc'))))\n",
    "\n",
    "print(svm_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use best params\n",
    "train_scaled = scaler.fit_transform(X_train_ave)\n",
    "\n",
    "svm_best = SVC(probability=True, kernel='linear', class_weight='balanced', C=1, decision_function_shape='ovo', random_state=39)\n",
    "    \n",
    "print(\"f1:\"+str(np.average(cross_val_score(svm_best, train_scaled, Y_train_class, scoring='f1'))))\n",
    "print(\"ROC_AUC:\"+str(np.average(cross_val_score(svm_best, train_scaled, Y_train_class, scoring='roc_auc'))))\n",
    "print(\"Accuracy:\"+str(np.average(cross_val_score(svm_best, train_scaled, Y_train_class, scoring='accuracy'))))\n",
    "\n",
    "svm_best.fit(train_scaled, Y_train_class)\n",
    "print(svm_best.score(train_scaled, Y_train_class))\n",
    "\n",
    "train_pred_svm = svm_best.predict(train_scaled)\n",
    "train_pred_prob_svm = svm_best.predict_proba(train_scaled)\n",
    "print(classification_report(Y_train_class, train_pred_svm))\n",
    "print(confusion_matrix(Y_train_class, train_pred_svm))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#grid search with cv for rf and ave features\n",
    "train_scaled = scaler.fit_transform(X_train_ave)\n",
    "\n",
    "param_grid = { \n",
    "    'n_estimators': [5, 10, 50, 100, 500],\n",
    "    'max_features': [None, 'sqrt', 'log2'],\n",
    "    'max_depth' : [3,4,5,6,7,None],\n",
    "    'criterion' :['gini', 'entropy'],\n",
    "    'bootstrap': [True, False]\n",
    "}\n",
    "\n",
    "scoring = ['accuracy', 'f1', 'precision', 'recall', 'roc_auc']\n",
    "\n",
    "rf_base = RandomForestClassifier(class_weight='balanced', random_state=39)\n",
    "\n",
    "rf_gs = GridSearchCV(rf_base, param_grid, cv=10, scoring = scoring, refit='f1')\n",
    "rf_gs.fit(train_scaled, Y_train_class)\n",
    "\n",
    "print(\"f1:\"+str(np.average(cross_val_score(rf_gs, train_scaled, Y_train_class, scoring='f1'))))\n",
    "print(\"ROC_AUC:\"+str(np.average(cross_val_score(rf_gs, train_scaled, Y_train_class, scoring='roc_auc'))))\n",
    "\n",
    "print(rf_gs.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#use best params\n",
    "train_scaled = scaler.fit_transform(X_train_ave)\n",
    "\n",
    "rf_best = RandomForestClassifier(class_weight='balanced', random_state=39)\n",
    "\n",
    "print(\"f1:\"+str(np.average(cross_val_score(rf_best, train_scaled, Y_train_class, scoring='f1'))))\n",
    "print(\"ROC_AUC:\"+str(np.average(cross_val_score(rf_best, train_scaled, Y_train_class, scoring='roc_auc'))))\n",
    "print(\"Accuracy:\"+str(np.average(cross_val_score(rf_best, train_scaled, Y_train_class, scoring='accuracy'))))\n",
    "\n",
    "rf_best.fit(train_scaled, Y_train_class)\n",
    "print(rf_best.score(train_scaled, Y_train_class))\n",
    "\n",
    "train_pred_rf = rf_best.predict(train_scaled)\n",
    "train_pred_prob_rf = rf_best.predict_proba(train_scaled)\n",
    "print(classification_report(Y_train_class, train_pred_rf))\n",
    "print(confusion_matrix(Y_train_class, train_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run on test data with best optimized model\n",
    "#scale data\n",
    "test_scaled = scaler.fit_transform(X_test_ave)\n",
    "\n",
    "print('GB test AUC: {}'.format(lr_cv.score(test_scaled, Y_test_class)))\n",
    "test_pred_gb = lr_cv.predict(test_scaled)\n",
    "test_pred_prob_gb = lr_cv.predict_proba(test_scaled)\n",
    "print(classification_report(Y_test_class, test_pred_gb))\n",
    "print(confusion_matrix(Y_test_class, test_pred_gb))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unsupervised clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# center and scale the data\n",
    "scaler = StandardScaler()\n",
    "\n",
    "features_clust_scaled = scaler.fit_transform(data[features_ave])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_range = range(2,10)\n",
    "scores = []\n",
    "for k in k_range:\n",
    "    km_ss = KMeans(n_clusters=k, random_state=1)\n",
    "    km_ss.fit(features_clust_scaled)\n",
    "    scores.append(silhouette_score(features_clust_scaled, km_ss.labels_))\n",
    "\n",
    "# plot the results\n",
    "plt.plot(k_range, scores)\n",
    "plt.xlabel('Number of clusters')\n",
    "plt.ylabel('Silhouette Coefficient')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "km2 = KMeans(n_clusters=2,random_state=1234)\n",
    "km2.fit(features_clust_scaled)\n",
    "data['kmeans_2_scaled'] = [ \"cluster_\" + str(label) for label in km2.labels_ ]\n",
    "data.groupby('kmeans_2_scaled').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.groupby('Severity')['kmeans_2_scaled'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
